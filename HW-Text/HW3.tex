\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\begin{document}

\title{APPM 5510 HW3}
\author{Zane Jakobs}
\date{}
\maketitle
\subsection*{1} With the notation $x_0^0$ representing the first element of $x_0$, we know from the values of $y$ that $x_0^0 + x_0^1 = 1,$ and that $x_1^0 + x_1^1 = 0$, and from the definition of the system that $x_1^0 = -x_0^0 + x_0^1$ and $x_1^1 = 2x_0^0 + x_0^1$. We plug these definitions into the equations generated by our observed data to get
\[
2x_0^1 = -x_0^0
\]
and thus
\[
\frac{1}{2} x_0^0 = 1- x_0^0 \implies x_0^0 = 2.
\]
We then have $x_0^1 = -\frac{1}{2} x_0^0 = -1,$ so $x_0 = (2,-1)^T.$
\subsection*{2} The controllability matrix
\[
[V] = \begin{bmatrix} 1 & 0\\  1 &0\end{bmatrix}
\]
does not have full row rank, so no, the system is not controllable.
\subsection*{3}
The Kalmain gain matrix here is 
\[
K = B\delta \dfrac{1}{\delta\cdot B \cdot \delta + \gamma^2} = \dfrac{\delta B}{\delta^2 B + \gamma^2}\tag{1}
\]
and the posterior KF covariance at some time $t$ is 
\[
C_t = (1-\delta K)B_t\tag{2}.
\]
Now, the background covariance updates as
\[
B_{t+1} = \lambda^2 C_t + \sigma^2, \tag{3}
\]
so by plugging (1) and (3) into (2), we get an update formula for the KF covariance:
\[
C_{t+1} = \left( 1  - \dfrac{\delta^2(\lambda^2 C_t +  \sigma^2)}{\delta^2(\lambda^2 C_t + \sigma^2) + \gamma^2}\right) (\lambda^2 C_t + \sigma^2).\tag{4}
\]
We then seek a fixed point of $C_t$ (by replacing $C_t$ and $C_{t+1}$ in (4) with just $C$ and solving algebraically) such that we can evaluate the limit as $\delta\to 0$ in the infinite-time limit. However, if we exchange the order of the limits (which, due to the linearity of limits, we can do if both limits exist), we see that
\[
\begin{aligned}
C &=  \left( 1  - \dfrac{0\cdot(\lambda^2 C +  \sigma^2)}{0\cdot(\lambda^2 C + \sigma^2) + \gamma^2}\right) (\lambda^2 C + \sigma^2)\\
&= \lambda^2 C + \sigma^2 \implies C = \dfrac{\sigma^2}{1-\lambda^2}.
\end{aligned}
\]
Thus, we see that the limiting value of $C$ does not depend at all on the properties of our observations, which makes sense because as $\delta\to 0$, the signal-to-noise ratio of our observations also goes to zero, and the system becomes non-observable. In such a non-observable system, the only factors that control the KF posterior covariance can come from our knowledge of the system dynamics, not our observations (which we cannot trust at all in this non-observable limit).
\subsection*{4(a)} The observability matrix,
\[
\begin{bmatrix}
1 & 0\\
\lambda & 0
\end{bmatrix}
\]
does not have full column rank, so no, this system is not observable.
\subsection*{4(b)} Assuming the i.i.d. $\eta_j$ have variance $\sigma^2$, the controllability matrix,
\[
[AV\ V] = \begin{bmatrix}
\lambda\sigma & 0 & \sigma & 0 \\
\sigma & 0 & 0 & 0
\end{bmatrix}
\]
has full row rank, so yes, the system is controllable.

\subsection*{4(c)} Since the observation operator selects only the more recent entry (e.g. $Hw_j = x_j$), the Kalman smoother's posterior variance updates as
\[
[C_{j-1} | C_j] = B_{j-1} - AB_{b-1} H^T(HB_jH^T + R)^{-1} HB_{j-1}A^T = B_{j-1} - \dfrac{(\lambda B_{j-1})^2}{B_j + \gamma^2}.
\]
Using the background covariance update formula from problem (3), we have a posterior smoother variance of
\[
[C_{j-1} | C_j] = (\lambda^2 C_{j-2} + \sigma^2)\left( 1 - \dfrac{\lambda^2 (\lambda^2 C_{j-2} + \sigma^2)}{\lambda^2 C_{j-1} + \sigma^2 + \gamma^2}\right).
\]
Seeking a fixed point of $C$, we end up at the equation $g(C) = C,$ where
\[
g(C) =  (\lambda^2 C + \sigma^2)\left( 1 - \dfrac{\lambda^2 (\lambda^2 C + \sigma^2)}{\lambda^2 C + \sigma^2 + \gamma^2}\right).
\]
Now, from LSZ 4.4.1, we know that the posterior variance of the first entry, which has update function 
\[
g_{4.4.1}(C) =  (\lambda^2 C + \sigma^2)\left( 1 - \dfrac{(\lambda^2 C + \sigma^2)}{\lambda^2 C + \sigma^2 + \gamma^2}\right),
\]
converges, so there does exist a solution to $g_{4.4.1}(C) = C$. Now, if $|\lambda | < 1$, then $g(C)$ gets closer (than $g_{4.4.1}(C)$) to equalling $\lambda^2 C + \sigma^2$ on iteration, as the portion of $g(C)$ in parentheses approaches 1 as $|\lambda |\to 0$ faster than that of $g_{4.4.1}(C)$, so by direct comparison, iterations of $g$ must converge in this case. If $|\lambda | = 1$ exactly, $g = g_{4.4.1}$, so since $g_{4.4.1}$ converges, $g$ must as well (since they are equal everywhere). Now, if $|\lambda | > 1$, since the part of $g$ in parentheses must be positive semi-definite (in order for it to sensibly represent a covariance), increasing $|\lambda |$ increases the negative part of that quantity, driving the term in parentheses to zero. Thus, since making $|\lambda |$ large makes $g$ smaller than if $|\lambda | \leq 1$, iterations of $g$ again must converge by direct comparison to $g_{4.4.1}$.

\end{document}
