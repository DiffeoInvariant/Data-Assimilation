\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amsfonts}
\begin{document}

\title{APPM 5510 HW2}
\author{Zane Jakobs}
\date{}
\maketitle

\subsection*{1(a)} 
The Bayesian update for the mean of this linear process can be obtained from the Kalman gain matrix
\[
\begin{aligned}
K &= BH^T(HBH^T + \gamma^2)^{-1}\\
&= \dfrac{1}{\sigma_1^2 + \gamma^2} \begin{pmatrix}\sigma_1^2 \\ \sigma_1\sigma_2 r\end{pmatrix}, \\
\end{aligned}
\]
whose action we then apply to the observation error term, 
\[
\begin{aligned}
\epsilon &= y - H\mathbf{\mu}\\
&= y - \mu_1.
\end{aligned}
\]
Thus, the Bayesian update is 
\[
\begin{aligned}
\begin{pmatrix}\mu_1 \\ \mu_2 \end{pmatrix} \to \ & \begin{pmatrix}\mu_1 \\ \mu_2 \end{pmatrix} + K\epsilon\\
			&= \begin{pmatrix}\mu_1 \\ \mu_2 \end{pmatrix} + \dfrac{(y-\mu_1)}{\sigma_1^2 + \gamma^2} \begin{pmatrix}\sigma_1^2 \\ \sigma_1\sigma_2 r\end{pmatrix}.
\end{aligned}
\]
\subsection*{1(b)} If $r = 0$, then the posterior mean is
\[
\begin{pmatrix}\mu_1 \\ \mu_2 \end{pmatrix} + \dfrac{\sigma_1^2(y-\mu_1)}{\sigma_1^2 + \gamma^2} \begin{pmatrix}1 \\ 0\end{pmatrix},
\]
and the unobserved variable that is now uncorrelated with the observed variable is not updated at all.
\subsection*{1(c(1))} As the measurement variance $\gamma$ goes to infinity, the posterior mean approaches the prior mean, with their difference going to zero as $O(\frac{1}{\gamma^2})$. This makes sense; as the measurement variance gets larger, we should "trust" the observation less, and hence update our beliefs less. 
\subsection*{1(c(2))} As the measurement variance goes to zero, the posterior mean becomes
\[
\begin{pmatrix}\mu_1 \\ \mu_2 \end{pmatrix} + \dfrac{(y-\mu_1)}{\sigma_1^2} \begin{pmatrix}\sigma_1^2 \\ \sigma_1\sigma_2 r\end{pmatrix},
\]
and thus the magnitude of the update to the mean depends on only on how uncertain we think the random variable is to begin with, as having zero observation error is essentially equivalent to directly sampling from the random variable.

\subsection*{1(c(3))} In this limit, the posterior mean is 
\[
\begin{pmatrix}\mu_1 \\ \mu_2 \end{pmatrix} + \dfrac{(y-\mu_1)}{\sigma_1^2 + \gamma^2} \begin{pmatrix}1 \\ r\end{pmatrix},
\]
so the magnitude of the update goes to zero linearly as $\sigma_1^2$ and $\sigma_2^2$ go to infinity.

\subsection*{2} First, the restriction that $X_a$ has mean $x$ implies that $x = \alpha x + \beta x\implies \ \alpha = 1-\beta.$ We now want to minimize $\mathrm{Var}(X_a) = \alpha^2 \mathrm{Var}(X_b) + \beta^2 \mathrm{Var}(Y) = (1-\beta)^2\sigma_x^2 + \beta^2\sigma_y^2$ subject to that constraint. We differentiate w.r.t. $\beta$ and set the expression equal to zero to get
\[
0 = \dfrac{\partial\mathrm{Var}(X_a)}{\partial\beta} = -2(1-\beta)\sigma_x^2 + 2\beta\sigma_y^2,
\]
which reduces to 
\[
\dfrac{\beta}{1-\beta} = \dfrac{\sigma_x^2}{\sigma_y^2} \implies \ \beta = \dfrac{\sigma_x^2/ \sigma_y^2}{1+\frac{\sigma_x^2}{\sigma_y^2}} = \dfrac{\sigma_x^2}{\sigma_y^2 + \sigma_x^2},
\]

and then $\alpha = 1 - \beta = 1 -  \dfrac{\sigma_x^2}{\sigma_y^2 + \sigma_x^2}$. Thus,

\[
X_a  \left( 1 - \dfrac{\sigma_x^2}{\sigma_y^2 + \sigma_x^2}\right) X_b +  \dfrac{\sigma_x^2}{\sigma_y^2 + \sigma_x^2} Y.
\]
The variance of $X_a$ can be obtained from the fact that 
\[
\begin{aligned}
\mathrm{Var}(X_a) &= \alpha^2\mathrm{Var}(X_b) + \beta^2 \mathrm{Var}(Y)\\
&= \left( 1 - \dfrac{\sigma_x^2}{\sigma_y^2 + \sigma_x^2}\right)^2 \sigma_x^2 + \left(\dfrac{\sigma_x^2}{\sigma_y^2 + \sigma_x^2}\right)^2\sigma_y^2.
\end{aligned}
\]

\subsection*{3(a)} Bayes' theorem gives us that the posterior pdf $[X | y]$ is (a constant times) $[y | x][X]$, which are easily obtained quantities. Specifically, we have 
\[
[y | x] = \exp(- |y - x - 1|)/2
\]
from the definition of $Y$, and the following prior on $X$:
\[
[X] = \dfrac{1}{c}\exp(-\frac{(x - \mu)^2}{\sigma_x^2}).
\]
This gives us the posterior distribution 
\[
\begin{aligned}
[X | y] &= \dfrac{1}{c} \exp(- |y - x - 1|)\exp(-\frac{(x - \mu)^2}{\sigma_x^2})\\
&= \dfrac{1}{c}\exp\left(-\frac{(x - \mu)^2}{\sigma_x^2} - |y - x - 1|\right).
\end{aligned}
\]

\subsection*{3 (b)} We seek to maximize $\exp\left(-\frac{x^2}{2} - |0.2 - x |\right)$ over $x$, which is obtained when the absolute value of the quantity in the exponent, $-\frac{x^2}{2} - |0.2 - x |$ (which is clearly negative definite), is minimized. This happens at $x = 0.2$, with can be justified either by noting that, since $x^2$ grows faster than $x$ when $|x| >> 0$, and slower than $x$ when $|x|\approx 0$, the minimum is obtained at the zero of the absolute value, or by numerically (or even symbolically!) maximizing with Mathematica (or whatever your preferred symbolic optimization software is). 

\end{document}
